{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), # resize the images to 224x24 pixels\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainnet.config import get_cfg_defaults\n",
    "\n",
    "cfg = get_cfg_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"subj01\"\n",
    "\n",
    "model_ckpt = '/home/admin/Algonaut/dat_colab_ckpt/{}/last.ckpt'.format(subj)\n",
    "cfg.DATASET.DATA_DIR = \"/home/admin/Algonaut/data/algonauts2023/{}\".format(subj)\n",
    "submission_dir = \"/home/admin/Algonaut/submission\"\n",
    "\n",
    "challenge_images = np.loadtxt(f\"/home/admin/Algonaut/data/ALG23/{subj}/image_ids/challenge_set.txt\", dtype=int)\n",
    "\n",
    "test_img_dir = '/home/admin/Algonaut/data/algonauts2023/{}/test_split/test_images'.format(subj)\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "idxs_test = np.arange(len(test_img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test, transform),\n",
    "    batch_size=6,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dat_models.dat import DAT\n",
    "backbone = DAT()\n",
    "backbone.load_state_dict(torch.load('/home/admin/Algonaut/dat_backbones/bkbn_upn_dat_b_160k.pth'))\n",
    "cfg.MODEL.LAYERS = list(range(4))\n",
    "cfg.MODEL.LAYER_WIDTHS = [128, 256, 512, 1024]\n",
    "cfg.MODEL.BOTTLENECK_DIM = 128  # can be reduced to speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainnet.plmodel import PLModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "plmodel = PLModel(\n",
    "    cfg, \n",
    "    backbone, \n",
    "    draw=False,  # draw on each epoch end\n",
    "    cached=False,  # cache the features into cpu memory in first epoch\n",
    ")\n",
    "# plmodel.validation_epoch_end() is called on validation epoch to draw\n",
    "plmodel = plmodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plmodel.load_state_dict(torch.load(model_ckpt)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 'subj01'\n",
    "fmri_dir = \"/home/admin2/Algonaut/data/algonauts2023/{}/training_split/training_fmri/\".format(subj)\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, \"lh_training_fmri.npy\"))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, \"rh_training_fmri.npy\"))\n",
    "print(lh_fmri.shape, rh_fmri.shape)\n",
    "lh_sample_submission = np.load('/home/admin2/Algonaut/algonauts_2023_challenge_submission/{}/lh_pred_test.npy'.format(subj))\n",
    "rh_sample_submission = np.load('/home/admin2/Algonaut/algonauts_2023_challenge_submission/{}/rh_pred_test.npy'.format(subj))\n",
    "print(lh_sample_submission.shape, rh_sample_submission.shape)\n",
    "lh_sample_test = np.load('/home/admin2/Algonaut/data/alg23/submission/lgr/{}/lh_pred_test.npy'.format(subj))\n",
    "rh_sample_test = np.load('/home/admin2/Algonaut/data/alg23/submission/lgr/{}/rh_pred_test.npy'.format(subj))\n",
    "print(lh_sample_test.shape, rh_sample_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test, transform),\n",
    "    batch_size=3,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "plmodel.eval()\n",
    "outs = []\n",
    "with torch.no_grad():\n",
    "    for img in test_imgs_dataloader:\n",
    "        img = img.cuda()\n",
    "        out, _ = plmodel(img)\n",
    "        outs.append(out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_np = np.vstack(outs)\n",
    "\n",
    "fmri_dir = \"/home/admin2/Algonaut/data/algonauts2023/{}/training_split/training_fmri/\".format(subj)\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, \"lh_training_fmri.npy\"))\n",
    "lh_fmri_len = lh_fmri.shape[1]\n",
    "\n",
    "lh = outs_np[:,:lh_fmri_len]\n",
    "rh = outs_np[:,lh_fmri_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_fmri_test_pred = lh.astype(np.float32)\n",
    "rh_fmri_test_pred = rh.astype(np.float32)\n",
    "np.save(os.path.join(submission_dir,'{}/lh_pred_test.npy'.format(subj)), lh_fmri_test_pred)\n",
    "np.save('/home/admin2/Algonaut/data/alg23/submission/lgr_dat/{}/rh_pred_test.npy'.format(subj), rh_fmri_test_pred)\n",
    "print('/home/admin2/Algonaut/data/alg23/submission/lgr_dat/{}/rh_pred_test.npy'.format(subj))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
